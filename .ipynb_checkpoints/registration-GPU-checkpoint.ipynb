{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import time,sys,glob,os\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndi\n",
    "import cc3d\n",
    "import cv2\n",
    "import torch\n",
    "import open3d as o3d\n",
    "\n",
    "from skimage import color, morphology, measure\n",
    "from skimage.transform import downscale_local_mean\n",
    "from skimage.registration import phase_cross_correlation\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from functions.cellregister import *\n",
    "from functions.iterive_non_rigid import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract_matches(lut_path, device):\n",
    "    \"\"\"Load LUT and extract matched coordinates and images with GPU support\"\"\"\n",
    "    with open(lut_path, 'rb') as f:\n",
    "        lookup_table = pickle.load(f)\n",
    "    \n",
    "    # Get images and convert to torch tensors\n",
    "    invivo_image = torch.tensor(lookup_table['in_vivo']['Transformed'], \n",
    "                              dtype=torch.float32).to(device)\n",
    "    exvivo_image = torch.tensor(lookup_table['exvivo_GCAMP']['Transformed'], \n",
    "                               dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Extract matched coordinates\n",
    "    in_vivo_coords = []\n",
    "    ex_vivo_coords = []\n",
    "    \n",
    "    for ex_cell in lookup_table['exvivo_GCAMP']['cells']:\n",
    "        if 'in_vivo_id' in ex_cell:\n",
    "            in_vivo_cell = next(cell for cell in lookup_table['in_vivo']['cells'] \n",
    "                               if cell['id'] == ex_cell['in_vivo_id'])\n",
    "            in_vivo_coords.append(in_vivo_cell['coordinates'])\n",
    "            ex_vivo_coords.append(ex_cell['coordinates'])\n",
    "    \n",
    "    # Convert coordinates to torch tensors\n",
    "    in_vivo_coords = torch.tensor(in_vivo_coords, dtype=torch.float32).to(device)\n",
    "    ex_vivo_coords = torch.tensor(ex_vivo_coords, dtype=torch.float32).to(device)\n",
    "    \n",
    "    return in_vivo_coords, ex_vivo_coords, invivo_image, exvivo_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rigid_transform(source_points, target_points):\n",
    "    \"\"\"Compute rigid transformation using GPU\"\"\"\n",
    "    # Center the point sets\n",
    "    source_centroid = torch.mean(source_points, dim=0)\n",
    "    target_centroid = torch.mean(target_points, dim=0)\n",
    "    \n",
    "    centered_source = source_points - source_centroid\n",
    "    centered_target = target_points - target_centroid\n",
    "    \n",
    "    # Compute optimal rotation\n",
    "    H = centered_source.T @ centered_target\n",
    "    U, _, V = torch.linalg.svd(H)\n",
    "    R = V.T @ U.T\n",
    "    \n",
    "    # Ensure right-handed coordinate system\n",
    "    if torch.linalg.det(R) < 0:\n",
    "        V[2, :] *= -1\n",
    "        R = V.T @ U.T\n",
    "    \n",
    "    # Compute scale\n",
    "    scale = torch.sum(centered_target * (R @ centered_source.T).T) / torch.sum(centered_source * centered_source)\n",
    "    \n",
    "    # Compute translation\n",
    "    t = target_centroid - scale * (R @ source_centroid)\n",
    "    \n",
    "    return R, t, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rigid_transform(image, target_image, R, t, scale=1.0):\n",
    "    \"\"\"Apply rigid transformation using GPU\"\"\"\n",
    "    # Convert inputs to torch tensors if they aren't already\n",
    "    if not torch.is_tensor(image):\n",
    "        image = torch.tensor(image, dtype=torch.float32).to(R.device)\n",
    "    if not torch.is_tensor(target_image):\n",
    "        target_image = torch.tensor(target_image, dtype=torch.float32).to(R.device)\n",
    "    \n",
    "    # Create affine grid\n",
    "    batch_size = 1\n",
    "    depth, height, width = target_image.shape\n",
    "    \n",
    "    # Create 4x4 transformation matrix\n",
    "    transform_matrix = torch.eye(4, device=R.device)\n",
    "    transform_matrix[:3, :3] = scale * R\n",
    "    transform_matrix[:3, 3] = t\n",
    "    \n",
    "    # Create normalized coordinate grid\n",
    "    grid_d, grid_h, grid_w = torch.meshgrid(\n",
    "        torch.linspace(-1, 1, depth, device=R.device),\n",
    "        torch.linspace(-1, 1, height, device=R.device),\n",
    "        torch.linspace(-1, 1, width, device=R.device)\n",
    "    )\n",
    "    \n",
    "    grid = torch.stack([grid_w, grid_h, grid_d, torch.ones_like(grid_w)], dim=-1)\n",
    "    grid = grid.reshape(-1, 4).T\n",
    "    \n",
    "    # Apply transformation\n",
    "    transformed_grid = transform_matrix @ grid\n",
    "    transformed_grid = transformed_grid[:3].T.reshape(depth, height, width, 3)\n",
    "    \n",
    "    # Normalize coordinates back to [-1, 1]\n",
    "    transformed_grid[..., 0] = transformed_grid[..., 0] * 2 / (width - 1) - 1\n",
    "    transformed_grid[..., 1] = transformed_grid[..., 1] * 2 / (height - 1) - 1\n",
    "    transformed_grid[..., 2] = transformed_grid[..., 2] * 2 / (depth - 1) - 1\n",
    "    \n",
    "    # Use grid_sample for interpolation\n",
    "    transformed = torch.nn.functional.grid_sample(\n",
    "        image.unsqueeze(0).unsqueeze(0),\n",
    "        transformed_grid.unsqueeze(0),\n",
    "        mode='bilinear',\n",
    "        padding_mode='zeros',\n",
    "        align_corners=True\n",
    "    )\n",
    "    \n",
    "    return transformed.squeeze()\n",
    "\n",
    "def learn_and_apply_deformable(source_img, target_img, vec_ds=3, device='cuda'):\n",
    "    \"\"\"GPU-accelerated non-rigid registration\"\"\"\n",
    "    ncc_list = []\n",
    "    vec_field_smooth_list = []\n",
    "    current_img = source_img.clone()\n",
    "    \n",
    "    # Convert to torch tensors if not already\n",
    "    if not torch.is_tensor(source_img):\n",
    "        source_img = torch.tensor(source_img, dtype=torch.float32).to(device)\n",
    "    if not torch.is_tensor(target_img):\n",
    "        target_img = torch.tensor(target_img, dtype=torch.float32).to(device)\n",
    "    \n",
    "    for i in range(30):\n",
    "        if i % 5 == 0:\n",
    "            print(f'iteration {i}...')\n",
    "        \n",
    "        # Move to CPU for phase correlation\n",
    "        current_cpu = current_img.cpu().numpy()\n",
    "        target_cpu = target_img.cpu().numpy()\n",
    "        \n",
    "        # Compute displacement field\n",
    "        shifts = torch.tensor(\n",
    "            phase_cross_correlation(current_cpu, target_cpu, \n",
    "                                  upsample_factor=vec_ds)[0],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Smooth displacement field\n",
    "        vec_field_smooth = torch.tensor(\n",
    "            gaussian_filter(shifts.cpu().numpy(), sigma=2),\n",
    "            device=device\n",
    "        )\n",
    "        vec_field_smooth_list.append(vec_field_smooth)\n",
    "        \n",
    "        # Apply displacement\n",
    "        for d in range(3):\n",
    "            current_img = torch.roll(current_img, \n",
    "                                   shifts=int(vec_field_smooth[d].item()), \n",
    "                                   dims=d)\n",
    "        \n",
    "        # Compute correlation\n",
    "        ncc = torch.corrcoef(current_img.flatten(), \n",
    "                            target_img.flatten())[0,1]\n",
    "        ncc_list.append(ncc.item())\n",
    "    \n",
    "    return current_img, source_img, ncc_list, vec_field_smooth_list\n",
    "    return current_img, source_img, ncc_list, vec_field_smooth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lut_path):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data and process\n",
    "    in_vivo_coords, ex_vivo_coords, invivo_image, exvivo_image = load_and_extract_matches(lut_path, device)\n",
    "    \n",
    "    # Compute and apply transformations\n",
    "    R, t, scale = compute_rigid_transform(in_vivo_coords, ex_vivo_coords)\n",
    "    rigid_transformed = apply_rigid_transform(invivo_image, exvivo_image, R, t, scale)\n",
    "    \n",
    "    # Non-rigid registration\n",
    "    final_image, _, ncc_list, _ = learn_and_apply_deformable(rigid_transformed, exvivo_image, device=device)\n",
    "    \n",
    "    # Move results back to CPU for visualization\n",
    "    final_image = final_image.cpu().numpy()\n",
    "    R = R.cpu().numpy()\n",
    "    t = t.cpu().numpy()\n",
    "    \n",
    "    return R, t, scale, final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LUT_multimodal_487_Region2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     lut_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLUT_multimodal_487_Region2.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     R, t, scale, registered_image \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlut_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(lut_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load data and process\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m in_vivo_coords, ex_vivo_coords, invivo_image, exvivo_image \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_extract_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlut_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compute and apply transformations\u001b[39;00m\n\u001b[1;32m     10\u001b[0m R, t, scale \u001b[38;5;241m=\u001b[39m compute_rigid_transform(in_vivo_coords, ex_vivo_coords)\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mload_and_extract_matches\u001b[0;34m(lut_path, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_extract_matches\u001b[39m(lut_path, device):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load LUT and extract matched coordinates and images with GPU support\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlut_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         lookup_table \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Get images and convert to torch tensors\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/jl10897/miniconda3/envs/Auto_R/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LUT_multimodal_487_Region2.pkl'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lut_path = \"/scratch/jl10897/Automatic_Registration/LUT_multimodal_487_Region2.pkl\"\n",
    "    R, t, scale, registered_image = main(lut_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
